....................................................................................
			  IO apis
....................................................................................
1.file system io
   file system io , how to read data from disk file
2.network io

File System IO:
=>We can read and write files from the disk in  two ways
  1.blocking way
  2.nonblocking way
=>We can read and write files using two mode
  1.NonStreaming mode
  2.Streaming mode
=>All file operations are handled by
  "Worker Threads" from Worker Thread Pool - either it is blocking or non blocking    io.
=>Files are handled using callback style or promise style.
=>Files operations are handled by "fs" module

......................................................................................................................................................................................................................................
											 File system operations
.....................................................................................................................................................................................................................................

1.create,read,write,update,delete,rename files and directories

Two styles:

1.callback style
	 require('node:fs')
2.promise style
        require('node:fs/promises')

Async                                            Sync
fs.readFile                                  fs.readFileSync
fs.writeFile                                  fs.writeFileSync
fs.appendFile                             fs.appendFileSync
fs.unlink                                      fs.unlinkSync   -- delete file
etc...

Async Read and Write:

How to read File using nonblocking pattern? using callbacks

fs.readFile(path[, options], callback)

path <string> | <Buffer> | <URL> | <integer> filename or file descriptor
options <Object> | <string>
 encoding <string> | <null> Default: null
 flag <string> See support of file system flags. Default: 'r'.
 signal <AbortSignal> allows aborting an in-progress readFile

callback <Function>
  err <Error> | <AggregateError>
  data <string> | <Buffer>

const fs = require('node:fs')

function blockMe(message) {
    console.log(message)
}

function main() {
    blockMe('start')
    const filePath = './src/assets/info.txt'
    const options = {
        encoding: 'UTF-8'
    }
    fs.readFile(filePath, options, (err, data) => {
        if (err) throw err
        console.log(data)
    })
    blockMe('end')
}
main()
***********************************************************************************************************************************************************
											Create/Write a File
***********************************************************************************************************************************************************

const fs = require('node:fs')
function createNewFile() {
    const filePath = './src/assets/demo.txt'
    const content = 'This is sample demo'
    fs.writeFile(filePath, content, (err) => {
        if (err) throw err
        console.log(`File "${filePath}" created`)
    })
}
function main() {
    createNewFile()
}
main()
*********************************************************************************************************************************************************
										 File append

Use case: Custom Logger in node that writes log messages to a file using api

fs.writeFile
fs.appendFile

You can log
info
Errors
Timestamps

const fs = require('node:fs')
const logFile = './app.log'
function logMessage(level, message) {
    const timeStamp = new Date().toISOString()
    const fullMessage = `[${timeStamp}] [${level.toUpperCase()}] ${message} \n`
    //append file
    fs.appendFile(logFile, fullMessage, (err) => {
        if (err) {
            console.log(`X Failed to write log`, err)
        }
    })
}
function info(msg) {
    logMessage('info', msg)
}
function error(msg) {
    logMessage('error', msg)
}
function warn(msg) {
    logMessage('warn', msg)
}
function main() {
    info('Web Server started on Port 3000')
    info('Database Server started on Port 1434')
    error('Unable to Start Message Broker')
    warn('High Memory Usage has been detected')
}
main()
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
									 File System Operations Using Promises
****************************************************************************************************************************************************
fs.readFile, fs.writeFile,fs.appendFile are callback based apis

How to convert these apis into promise?

1.Using custom promises - you convert the callback apis into promises
2.Using node:fs/promises package


Custom Promise:
const fs = require('node:fs')

//callback based
// function readTextFile() {
//     const filePath = './src/assets/info.txt'
//     const options = {
//         encoding: 'UTF-8'
//     }
//     fs.readFile(filePath, options, (err, data) => {
//         if (err) throw err
//         console.log(data)
//     })
// }

async function getValue(){
    return 10 // Promise.resolve(10)
}
async function readTextFile() {
    return new Promise((resolve, reject) => {
        const filePath = './src/assets/info.txt'
        const options = {
            encoding: 'UTF-8'
        }
        fs.readFile(filePath, options, (err, data) => {
            if (err) {
                reject(err)
            } else {
                resolve(data)
            }
        })
    })
}

async function main() {
    //  readTextFile().then(data=>console.log(data)).catch(err=>console.log(err))
    try {
        const data = await readTextFile()
        console.log(data)
    }
    catch (err) {
        console.log(err)
    }
}
main()
******************
Promise Apis:
......................
const fs = require('node:fs/promises')

async function readTextFile() {
    const filePath = './src/assets/info.txt'
    const options = {
        encoding: 'UTF-8'
    }
    // fs.readFile(filePath, options)
    //     .then(data => console.log(data))
    //     .catch(err => console.log(err))
    try {
        const data = await fs.readFile(filePath, options)
        console.log(data)
    }
    catch (err) {
        console.log(Error)
    }
}

function main() {
    readTextFile()
}
main()
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
										  File Path And path module
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
The path module provides utilities for working with file and directory paths.

-node provides lot of global variables

__dirname  : current directory name
__filename : current directory name + fileName

const fs = require('node:fs/promises')
const path = require('node:path')

async function readTextFile() {
    //const filePath = './src/assets/info.txt'
    const filePath = path.join(__dirname, 'assets/info.txt')
    const options = {
        encoding: 'UTF-8'
    }
    try {
        const data = await fs.readFile(filePath, options)
        console.log(data)
    }
    catch (err) {
        console.log(Error)
    }
}

function main() {
    readTextFile()
}
main()
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
										Blocking File operations
									 	   (fs.readFileSync)
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

fs.readFileSync(filePath,options)

const fs = require('node:fs')
const path = require('node:path')

async function readTextFile() {
    const filePath = path.join(__dirname, 'assets/info.txt')
    const options = {
        encoding: 'UTF-8'
    }
    console.log('start')
    const data = fs.readFileSync(filePath,options)
    console.log(data)
    console.log('end')
}

function main() {
    readTextFile()
}
main()
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
										 File Read and Write Mode
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

1.Non Streaming Mode
2.Streaming Mode

		Differences between streaming(fs.createReadStream)    and Non Streaming (fs.readFile())

 Features                                    fs.readFile                                                                      fs.createReadStream

 ReadFile                                   All at once(entire file in Memory)                                  In small Chunks(streamed)

Memory Usage                         High for large files                                                          Low and efficient for large files

Callback based                         Yes                                                                                  Event based - data, end , error

Blocking nature                         Async, but still waits for whole file                                  streams data progressively

Best For                                     Small config/data files                                                     Large files like logs, media 

Types of Streams:

1.Readable Stream : input
2.Writeable stream : output
3.Duplex stream : read + write

Node has lot of built in stream apis
....................................

Built in readable Streams:

-HTTP responses, on the client
-HTTP requests, on the server
-fs read streams
-zlib streams
-crypto streams
-TCP sockets
-child process stdout and stderr
-process.stdin

Writable Streams:

-HTTP requests, on the client
-HTTP responses, on the server
-fs write streams
-zlib streams
-crypto streams
-TCP sockets
-child process stdin
-process.stdout, process.stderr

All streaming apis are powered with events
node io streams has built in events.
events are emitted by node.
Our programs are listeners


Common events in all io
.........................
1.data event:
 which is emitted by node, for each chunk.

2.close event:
  The 'close' event is emitted when the stream and any of its underlying resources (a file descriptor, for example) have been closed.

3.end event:
 The 'end' event is emitted when there is no more data to be consumed from the stream.

4.Event: 'error'
 The 'error' event may be emitted by a Readable implementation at any time
Typically, this may occur if the underlying stream is unable to generate data due to an underlying internal failure, or when a stream implementation attempts to push an invalid chunk of data.


const fs = require('node:fs')
const path = require('node:path')

function readTextFile() {
    const filePath = path.join(__dirname, 'assets/info.txt')
    const options = {
        encoding: 'UTF-8'
    }
    const inputstream = fs.createReadStream(filePath, options)

    //register events for reading data
    let data=''
    inputstream.on('data', (chunk) => {
        //console.log(chunk)
        data+=chunk
    })
    inputstream.on('end', () => {
        console.log('there is no more')
        console.log(data)
    })
    inputstream.on('close', () => {
        console.log('close ')
    })
    inputstream.on('error', (err) => {
        console.log(err)
    })
}
function main() {
    readTextFile()
}
main()
*****************************************************************************

Write File:
..................

const fs = require('node:fs')
const path = require('node:path')

function write() {
    let filePath = path.join(__dirname, 'assets/todos.json')
    const config = {
        encoding: 'utf8',
        flag: 'w'
    };
    const outputStream = fs.createWriteStream(filePath, config)
    //data
    const todos = [{ text: 'learn Js', status: 'completed' }, { text: 'learn node', status: 'in Progress' }]
    let jsonTodos = JSON.stringify(todos)


    outputStream.write(jsonTodos)
    //attach events
    outputStream.close();

    outputStream.on('close', function () {
        console.log('file has been written ')
    })

}
function main(){
    write()
}
main()
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
										Back pressure
*******************************************************************************************************************************************************
Backpressure is a mechanism for controlling data flow between a fast producer (reading file) and slow consumer( writing to a network,file or any stream).
 Problems when you do read and write together

1. In general read operation is faster than write operation

			Back Pressure means inputstream is fast, outputstream slow, then data will be lost.

Without Back Pressure Handling

1.Producer sends data too fast
2.The consumer cant process it quickly enough
3.Memory fills up - > crash or degraded performance

With Back Pressure Handling
- The producer pauses when the consumer internal buffer is full
->it resumes when the consumer is ready again

How to handle back pressure?

 apis  : pause ,resume, drain event
pause : to close the upstream, not to emit data
resume : to open the open upstream , to emit data
drain event: if drain event is called, means buffer is empty

Before Testing BackPressure , we need bigFile

//big file creation
const fs = require('node:fs');
const path = require('node:path')

const filePath = path.join(__dirname, "assets/big.file")

const file = fs.createWriteStream(filePath);

for (let i = 0; i <= 1e6; i++) {
    file.write('Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n');
}

How to handle back pressure using pause,resume,drain?

const fs = require('node:fs');
const path = require('node:path');

const inputfileName = path.join(__dirname, 'assets/big.file');
const outputfileName = path.join(__dirname, 'assets/bigcopy.file');

const config = {
    encoding: 'UTF-8'
}
const readerStream = fs.createReadStream(inputfileName, config);
const writeStr = fs.createWriteStream(outputfileName, config);

readerStream.on('data', function (chunk) {
    console.log(`Received ${chunk.length} bytes of data.`);
    let buffer_good = writeStr.write(chunk);
    if (!buffer_good) readerStream.pause();
});
writeStr.on('drain', function () {
    console.log('buffer drained!');
    readerStream.resume();
});
readerStream.on('end', function () {
    // console.log(data);
});

readerStream.on('error', function (err) {
    console.log(err.stack);
});	
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
									Backpressure Handling using pipe
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
Pipe method to eliminate backpressure apis(drain,resume,pause)
const fs = require('node:fs');
const path = require('node:path');

const inputfileName = path.join(__dirname, 'assets/big.file');
//write
const outputFileName = path.join(__dirname, 'assets/bigcopy.file');

const config = {
      encoding: 'UTF-8'
}

//Back pressure handling
const readerStream = fs.createReadStream(inputfileName, config);
const writeStr = fs.createWriteStream(outputFileName, config);

//backPressure streams
//pipe method is simplest method which wraps resume,pasuse,drain 
readerStream.pipe(writeStr);
&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&